{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553df376",
   "metadata": {},
   "source": [
    "# Transforming files from CVS to TFRecords\n",
    "Here we are transforming our test and training data to TFRecords, so that we can use it for model training. TFRecords' binary format is more efficient to read than the text in the original csv file, so this helps our model be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded97a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc41947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import contextlib2\n",
    "from object_detection.dataset_tools import tf_record_creation_util\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f158e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a86c24cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install contextlib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db120a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335616, 416)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>reactivity_0001</th>\n",
       "      <th>reactivity_0002</th>\n",
       "      <th>reactivity_0003</th>\n",
       "      <th>reactivity_0004</th>\n",
       "      <th>reactivity_0005</th>\n",
       "      <th>reactivity_0006</th>\n",
       "      <th>...</th>\n",
       "      <th>reactivity_error_0197</th>\n",
       "      <th>reactivity_error_0198</th>\n",
       "      <th>reactivity_error_0199</th>\n",
       "      <th>reactivity_error_0200</th>\n",
       "      <th>reactivity_error_0201</th>\n",
       "      <th>reactivity_error_0202</th>\n",
       "      <th>reactivity_error_0203</th>\n",
       "      <th>reactivity_error_0204</th>\n",
       "      <th>reactivity_error_0205</th>\n",
       "      <th>reactivity_error_0206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000d87cab97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_2A3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000d87cab97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_DMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001ca9d21b0</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001ca9d21b0</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00021f968267</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_Replicates_from_previous_l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sequence_id                                           sequence  \\\n",
       "0  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "1  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "2  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "3  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "4  00021f968267  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n",
       "\n",
       "  experiment_type                                       dataset_name  \\\n",
       "0         2A3_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_2A3   \n",
       "1         DMS_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_DMS   \n",
       "2         2A3_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3   \n",
       "3         DMS_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS   \n",
       "4         2A3_MaP  DasLabBigLib_OneMil_Replicates_from_previous_l...   \n",
       "\n",
       "   reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
       "0              NaN              NaN              NaN              NaN   \n",
       "1              NaN              NaN              NaN              NaN   \n",
       "2              NaN              NaN              NaN              NaN   \n",
       "3              NaN              NaN              NaN              NaN   \n",
       "4              NaN              NaN              NaN              NaN   \n",
       "\n",
       "   reactivity_0005  reactivity_0006  ...  reactivity_error_0197  \\\n",
       "0              NaN              NaN  ...                    NaN   \n",
       "1              NaN              NaN  ...                    NaN   \n",
       "2              NaN              NaN  ...                    NaN   \n",
       "3              NaN              NaN  ...                    NaN   \n",
       "4              NaN              NaN  ...                    NaN   \n",
       "\n",
       "   reactivity_error_0198  reactivity_error_0199  reactivity_error_0200  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0201  reactivity_error_0202  reactivity_error_0203  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0204  reactivity_error_0205  reactivity_error_0206  \n",
       "0                    NaN                    NaN                    NaN  \n",
       "1                    NaN                    NaN                    NaN  \n",
       "2                    NaN                    NaN                    NaN  \n",
       "3                    NaN                    NaN                    NaN  \n",
       "4                    NaN                    NaN                    NaN  \n",
       "\n",
       "[5 rows x 416 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading in the train data\n",
    "\n",
    "df_train = pd.read_csv('train_data_QUICK_START.csv')\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "140b61e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 1, 'C': 2, 'G': 3, 'U': 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dictionary for encoding the RNA sequence letters into numbers \n",
    "\n",
    "train = df_train.sequence.to_numpy()\n",
    "encoding_dict = {'A':1, 'C': 2, 'G': 3, 'U': 4}\n",
    "encoding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371f5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel links we used in creating this notebook:\n",
    "# https://www.kaggle.com/code/irohith/aslfr-preprocess-dataset/notebook\n",
    "# https://www.kaggle.com/code/konstantinboyko/convert-original-csv-file-to-tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db779f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the 206 column names for reactivity and reactivity errors into lists, to later use the columns as a whole\n",
    "\n",
    "lo_react_cols = df_train.filter(like='reactivity_0').columns\n",
    "lo_error_cols = df_train.filter(like='reactivity_error_0').columns\n",
    "\n",
    "lst_react_cols = lo_react_cols.to_list()\n",
    "lst_error_cols = lo_error_cols.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ca39a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reactivity_0001', 'reactivity_0002', 'reactivity_0003', 'reactivity_0004', 'reactivity_0005', 'reactivity_0006', 'reactivity_0007', 'reactivity_0008', 'reactivity_0009', 'reactivity_0010', 'reactivity_0011', 'reactivity_0012', 'reactivity_0013', 'reactivity_0014', 'reactivity_0015', 'reactivity_0016', 'reactivity_0017', 'reactivity_0018', 'reactivity_0019', 'reactivity_0020', 'reactivity_0021', 'reactivity_0022', 'reactivity_0023', 'reactivity_0024', 'reactivity_0025', 'reactivity_0026', 'reactivity_0027', 'reactivity_0028', 'reactivity_0029', 'reactivity_0030', 'reactivity_0031', 'reactivity_0032', 'reactivity_0033', 'reactivity_0034', 'reactivity_0035', 'reactivity_0036', 'reactivity_0037', 'reactivity_0038', 'reactivity_0039', 'reactivity_0040', 'reactivity_0041', 'reactivity_0042', 'reactivity_0043', 'reactivity_0044', 'reactivity_0045', 'reactivity_0046', 'reactivity_0047', 'reactivity_0048', 'reactivity_0049', 'reactivity_0050', 'reactivity_0051', 'reactivity_0052', 'reactivity_0053', 'reactivity_0054', 'reactivity_0055', 'reactivity_0056', 'reactivity_0057', 'reactivity_0058', 'reactivity_0059', 'reactivity_0060', 'reactivity_0061', 'reactivity_0062', 'reactivity_0063', 'reactivity_0064', 'reactivity_0065', 'reactivity_0066', 'reactivity_0067', 'reactivity_0068', 'reactivity_0069', 'reactivity_0070', 'reactivity_0071', 'reactivity_0072', 'reactivity_0073', 'reactivity_0074', 'reactivity_0075', 'reactivity_0076', 'reactivity_0077', 'reactivity_0078', 'reactivity_0079', 'reactivity_0080', 'reactivity_0081', 'reactivity_0082', 'reactivity_0083', 'reactivity_0084', 'reactivity_0085', 'reactivity_0086', 'reactivity_0087', 'reactivity_0088', 'reactivity_0089', 'reactivity_0090', 'reactivity_0091', 'reactivity_0092', 'reactivity_0093', 'reactivity_0094', 'reactivity_0095', 'reactivity_0096', 'reactivity_0097', 'reactivity_0098', 'reactivity_0099', 'reactivity_0100', 'reactivity_0101', 'reactivity_0102', 'reactivity_0103', 'reactivity_0104', 'reactivity_0105', 'reactivity_0106', 'reactivity_0107', 'reactivity_0108', 'reactivity_0109', 'reactivity_0110', 'reactivity_0111', 'reactivity_0112', 'reactivity_0113', 'reactivity_0114', 'reactivity_0115', 'reactivity_0116', 'reactivity_0117', 'reactivity_0118', 'reactivity_0119', 'reactivity_0120', 'reactivity_0121', 'reactivity_0122', 'reactivity_0123', 'reactivity_0124', 'reactivity_0125', 'reactivity_0126', 'reactivity_0127', 'reactivity_0128', 'reactivity_0129', 'reactivity_0130', 'reactivity_0131', 'reactivity_0132', 'reactivity_0133', 'reactivity_0134', 'reactivity_0135', 'reactivity_0136', 'reactivity_0137', 'reactivity_0138', 'reactivity_0139', 'reactivity_0140', 'reactivity_0141', 'reactivity_0142', 'reactivity_0143', 'reactivity_0144', 'reactivity_0145', 'reactivity_0146', 'reactivity_0147', 'reactivity_0148', 'reactivity_0149', 'reactivity_0150', 'reactivity_0151', 'reactivity_0152', 'reactivity_0153', 'reactivity_0154', 'reactivity_0155', 'reactivity_0156', 'reactivity_0157', 'reactivity_0158', 'reactivity_0159', 'reactivity_0160', 'reactivity_0161', 'reactivity_0162', 'reactivity_0163', 'reactivity_0164', 'reactivity_0165', 'reactivity_0166', 'reactivity_0167', 'reactivity_0168', 'reactivity_0169', 'reactivity_0170', 'reactivity_0171', 'reactivity_0172', 'reactivity_0173', 'reactivity_0174', 'reactivity_0175', 'reactivity_0176', 'reactivity_0177', 'reactivity_0178', 'reactivity_0179', 'reactivity_0180', 'reactivity_0181', 'reactivity_0182', 'reactivity_0183', 'reactivity_0184', 'reactivity_0185', 'reactivity_0186', 'reactivity_0187', 'reactivity_0188', 'reactivity_0189', 'reactivity_0190', 'reactivity_0191', 'reactivity_0192', 'reactivity_0193', 'reactivity_0194', 'reactivity_0195', 'reactivity_0196', 'reactivity_0197', 'reactivity_0198', 'reactivity_0199', 'reactivity_0200', 'reactivity_0201', 'reactivity_0202', 'reactivity_0203', 'reactivity_0204', 'reactivity_0205', 'reactivity_0206']\n",
      "['reactivity_error_0001', 'reactivity_error_0002', 'reactivity_error_0003', 'reactivity_error_0004', 'reactivity_error_0005', 'reactivity_error_0006', 'reactivity_error_0007', 'reactivity_error_0008', 'reactivity_error_0009', 'reactivity_error_0010', 'reactivity_error_0011', 'reactivity_error_0012', 'reactivity_error_0013', 'reactivity_error_0014', 'reactivity_error_0015', 'reactivity_error_0016', 'reactivity_error_0017', 'reactivity_error_0018', 'reactivity_error_0019', 'reactivity_error_0020', 'reactivity_error_0021', 'reactivity_error_0022', 'reactivity_error_0023', 'reactivity_error_0024', 'reactivity_error_0025', 'reactivity_error_0026', 'reactivity_error_0027', 'reactivity_error_0028', 'reactivity_error_0029', 'reactivity_error_0030', 'reactivity_error_0031', 'reactivity_error_0032', 'reactivity_error_0033', 'reactivity_error_0034', 'reactivity_error_0035', 'reactivity_error_0036', 'reactivity_error_0037', 'reactivity_error_0038', 'reactivity_error_0039', 'reactivity_error_0040', 'reactivity_error_0041', 'reactivity_error_0042', 'reactivity_error_0043', 'reactivity_error_0044', 'reactivity_error_0045', 'reactivity_error_0046', 'reactivity_error_0047', 'reactivity_error_0048', 'reactivity_error_0049', 'reactivity_error_0050', 'reactivity_error_0051', 'reactivity_error_0052', 'reactivity_error_0053', 'reactivity_error_0054', 'reactivity_error_0055', 'reactivity_error_0056', 'reactivity_error_0057', 'reactivity_error_0058', 'reactivity_error_0059', 'reactivity_error_0060', 'reactivity_error_0061', 'reactivity_error_0062', 'reactivity_error_0063', 'reactivity_error_0064', 'reactivity_error_0065', 'reactivity_error_0066', 'reactivity_error_0067', 'reactivity_error_0068', 'reactivity_error_0069', 'reactivity_error_0070', 'reactivity_error_0071', 'reactivity_error_0072', 'reactivity_error_0073', 'reactivity_error_0074', 'reactivity_error_0075', 'reactivity_error_0076', 'reactivity_error_0077', 'reactivity_error_0078', 'reactivity_error_0079', 'reactivity_error_0080', 'reactivity_error_0081', 'reactivity_error_0082', 'reactivity_error_0083', 'reactivity_error_0084', 'reactivity_error_0085', 'reactivity_error_0086', 'reactivity_error_0087', 'reactivity_error_0088', 'reactivity_error_0089', 'reactivity_error_0090', 'reactivity_error_0091', 'reactivity_error_0092', 'reactivity_error_0093', 'reactivity_error_0094', 'reactivity_error_0095', 'reactivity_error_0096', 'reactivity_error_0097', 'reactivity_error_0098', 'reactivity_error_0099', 'reactivity_error_0100', 'reactivity_error_0101', 'reactivity_error_0102', 'reactivity_error_0103', 'reactivity_error_0104', 'reactivity_error_0105', 'reactivity_error_0106', 'reactivity_error_0107', 'reactivity_error_0108', 'reactivity_error_0109', 'reactivity_error_0110', 'reactivity_error_0111', 'reactivity_error_0112', 'reactivity_error_0113', 'reactivity_error_0114', 'reactivity_error_0115', 'reactivity_error_0116', 'reactivity_error_0117', 'reactivity_error_0118', 'reactivity_error_0119', 'reactivity_error_0120', 'reactivity_error_0121', 'reactivity_error_0122', 'reactivity_error_0123', 'reactivity_error_0124', 'reactivity_error_0125', 'reactivity_error_0126', 'reactivity_error_0127', 'reactivity_error_0128', 'reactivity_error_0129', 'reactivity_error_0130', 'reactivity_error_0131', 'reactivity_error_0132', 'reactivity_error_0133', 'reactivity_error_0134', 'reactivity_error_0135', 'reactivity_error_0136', 'reactivity_error_0137', 'reactivity_error_0138', 'reactivity_error_0139', 'reactivity_error_0140', 'reactivity_error_0141', 'reactivity_error_0142', 'reactivity_error_0143', 'reactivity_error_0144', 'reactivity_error_0145', 'reactivity_error_0146', 'reactivity_error_0147', 'reactivity_error_0148', 'reactivity_error_0149', 'reactivity_error_0150', 'reactivity_error_0151', 'reactivity_error_0152', 'reactivity_error_0153', 'reactivity_error_0154', 'reactivity_error_0155', 'reactivity_error_0156', 'reactivity_error_0157', 'reactivity_error_0158', 'reactivity_error_0159', 'reactivity_error_0160', 'reactivity_error_0161', 'reactivity_error_0162', 'reactivity_error_0163', 'reactivity_error_0164', 'reactivity_error_0165', 'reactivity_error_0166', 'reactivity_error_0167', 'reactivity_error_0168', 'reactivity_error_0169', 'reactivity_error_0170', 'reactivity_error_0171', 'reactivity_error_0172', 'reactivity_error_0173', 'reactivity_error_0174', 'reactivity_error_0175', 'reactivity_error_0176', 'reactivity_error_0177', 'reactivity_error_0178', 'reactivity_error_0179', 'reactivity_error_0180', 'reactivity_error_0181', 'reactivity_error_0182', 'reactivity_error_0183', 'reactivity_error_0184', 'reactivity_error_0185', 'reactivity_error_0186', 'reactivity_error_0187', 'reactivity_error_0188', 'reactivity_error_0189', 'reactivity_error_0190', 'reactivity_error_0191', 'reactivity_error_0192', 'reactivity_error_0193', 'reactivity_error_0194', 'reactivity_error_0195', 'reactivity_error_0196', 'reactivity_error_0197', 'reactivity_error_0198', 'reactivity_error_0199', 'reactivity_error_0200', 'reactivity_error_0201', 'reactivity_error_0202', 'reactivity_error_0203', 'reactivity_error_0204', 'reactivity_error_0205', 'reactivity_error_0206']\n"
     ]
    }
   ],
   "source": [
    "print(lst_react_cols)\n",
    "print(lst_error_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fcb532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the now redundant columns\n",
    "del lo_error_cols, lo_react_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "850599ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding all the reactivities and errors into two big columns\n",
    "# Now there is one big list for each row including the columns of reactivities, the same for reactivity errors\n",
    "df_train['reactivity'] = df_train[lst_react_cols].values.tolist()\n",
    "df_train['error'] = df_train[lst_error_cols].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3802a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.023, -0.039, 0.114, 0.019, 0.171, -0.162, 0.094, 0.0, 0.247, -0.44, 0.718, 0.361, 0.094, 0.38, 0.749, 0.468, 0.571, 2.759, 1.37, -0.23, -0.804, -0.002, 0.245, -0.023, -0.04, 0.076, -0.039, 0.133, -0.287, 0.227, 0.228, -0.153, 0.0, 0.057, 0.0, -0.134, 0.0, 0.0, 0.571, 0.571, 0.352, 0.751, 0.437, 0.19, 0.114, 0.076, 0.19, 0.076, 0.019, 0.247, 1.389, 0.571, 1.427, 1.269, 0.272, 0.037, 1.046, 1.484, 0.856, 0.0, -0.439, -0.096, 0.057, 0.305, -0.096, -0.02, -0.02, 0.375, 0.788, 0.266, -0.038, -0.345, -0.65, -1.223, -0.096, 0.057, 0.306, 0.241, 0.368, -0.478, -0.153, 1.087, 1.213, 0.693, 0.057, 0.457, 1.673, 0.328, 1.139, 0.633, 0.724, 1.201, 0.648, 1.201, 2.153, 0.36, 0.876, 0.298, 0.355, 0.074, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "# Example of the 'reactivity' column\n",
    "print(df_train['reactivity'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9478c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the previous separate columns for reactivity/errors\n",
    "\n",
    "df_train.drop(columns=lst_react_cols,inplace=True)\n",
    "df_train.drop(columns=lst_error_cols,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "555581e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting indexes for all the columns\n",
    "\n",
    "all_cols = df_train.columns\n",
    "\n",
    "idx_seq_id          = all_cols.get_loc('sequence_id')\n",
    "idx_sequence        = all_cols.get_loc('sequence')\n",
    "idx_dataset_name    = all_cols.get_loc('dataset_name')\n",
    "idx_reactivity      = all_cols.get_loc('reactivity')\n",
    "idx_error           = all_cols.get_loc('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31e875a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the DMS_Map and 2A3_Map rows into two separate datasets\n",
    "\n",
    "df_DMS = df_train.loc[df_train.experiment_type=='DMS_MaP'].reset_index(drop=True)\n",
    "df_2A3 = df_train.loc[df_train.experiment_type=='2A3_MaP'].reset_index(drop=True)\n",
    "\n",
    "# Delete the original dataset, now we have two separate ones instead !\n",
    "del df_train\n",
    "\n",
    "# Garbage collector time ! Free up memory no longer in use\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66fdaebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for converting dataset values into TensorFlow features, to prepare data for storing it in a TFRecord file\n",
    "\n",
    "def int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def int64_list_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def bytes_list_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "def float_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def float_list_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def string_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
    "\n",
    "def string_list_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value.encode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1b50af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating a tf.train.Example message for writing it into a TFRecord file later\n",
    "\n",
    "def serialize_row(row_2A3, row_DMS):\n",
    "  \n",
    "    # Check if the row indexes and sequence indexes are matching \n",
    "    assert row_2A3[idx_seq_id] == row_DMS[idx_seq_id] and row_2A3[idx_sequence] == row_DMS[idx_sequence]\n",
    "    \n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible data type\n",
    "    feature = {\n",
    "        \"id\": string_feature(row_2A3[idx_seq_id]),\n",
    "        \"seq\": float_list_feature([encoding_dict[char] for char in row_2A3[idx_sequence]]), \n",
    "\n",
    "        \"dataset_name_2A3\": string_feature(row_2A3[idx_dataset_name]),\n",
    "        \"reactivity_2A3\": float_list_feature(row_2A3[idx_reactivity]),\n",
    "        \"error_2A3\": float_list_feature(row_2A3[idx_error]),\n",
    "\n",
    "        \"dataset_name_DMS\": string_feature(row_DMS[idx_dataset_name]),\n",
    "        \"reactivity_DMS\": float_list_feature(row_DMS[idx_reactivity]),\n",
    "        \"error_DMS\": float_list_feature(row_DMS[idx_error])\n",
    "        }\n",
    "\n",
    "    # Create a Features message using tf.train.Example\n",
    "    features=tf.train.Features(feature=feature)\n",
    "\n",
    "    serialized_row = tf.train.Example(features=features)\n",
    "    return serialized_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c303f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for creating TFRecords of TF examples\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "def open_sharded_output_tfrecords(exit_stack, base_path, num_shards):\n",
    "    # Opens all TFRecord shards for writing and adds them to an exit stack.\n",
    "\n",
    "    # exit_stack: A context2.ExitStack used to automatically close the TFRecords opened in this function.\n",
    "    # base_path: The base folder for all the TFRecord files\n",
    "    # num_shards: The number of shards\n",
    "  \n",
    "    # Creating a proper filename to keep consistency in the folder\n",
    "    tf_record_filename = ['{}/{:03d}.tfrecord'.format(base_path, idx) for idx in range(num_shards)]\n",
    "\n",
    "    # options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "    tfrecords = [\n",
    "        exit_stack.enter_context(tf.io.TFRecordWriter(file_name, options=\"GZIP\"))\n",
    "            for file_name in tf_record_filename \n",
    "    ]\n",
    "    \n",
    "    # Returns the list of TFRecords\n",
    "    return tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71acd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the number of shards (we used 164) and a folder pathname for TFRecord files\n",
    "num_shards=164\n",
    "file_folder_path='tfrfile'\n",
    "\n",
    "# Creating the TFRecord files\n",
    "with contextlib2.ExitStack() as tf_record_close_stack:\n",
    "    output_tfrecords = open_sharded_output_tfrecords(tf_record_close_stack, file_folder_path, num_shards)\n",
    "    for index, (row_2A3, row_DMS) in enumerate(zip(df_2A3.itertuples(index=False), df_DMS.itertuples(index=False))):\n",
    "        tf_example = serialize_row(row_2A3, row_DMS)\n",
    "        shard_index = index % num_shards\n",
    "        output_tfrecords[shard_index].write(tf_example.SerializeToString())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25481bfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "DataLossError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} corrupted record at 0 (Is this even a TFRecord file?) [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(example)  \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Test the function on 001.tfrecord\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m parse_tfrecord(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfrfile/001.tfrecord\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m, in \u001b[0;36mparse_tfrecord\u001b[1;34m(tfrecord_file)\u001b[0m\n\u001b[0;32m      4\u001b[0m raw_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTFRecordDataset(tfrecord_file, compression_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Read only the first 5 records, since there are a lot of them there\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m raw_record \u001b[38;5;129;01min\u001b[39;00m raw_dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m):  \n\u001b[0;32m      8\u001b[0m     example \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mExample()\n\u001b[0;32m      9\u001b[0m     example\u001b[38;5;241m.\u001b[39mParseFromString(raw_record\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_internal()\n\u001b[0;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39miterator_get_next(\n\u001b[0;32m    774\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource,\n\u001b[0;32m    775\u001b[0m       output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[0;32m    776\u001b[0m       output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes)\n\u001b[0;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3029\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3027\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3028\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3029\u001b[0m   _ops\u001b[38;5;241m.\u001b[39mraise_from_not_ok_status(e, name)\n\u001b[0;32m   3030\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3031\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mDataLossError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} corrupted record at 0 (Is this even a TFRecord file?) [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "# Function for checking the insides of a tfrecord file \n",
    "\n",
    "def parse_tfrecord(tfrecord_file):\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_file, compression_type=None,)\n",
    "\n",
    "    # Read only the first 5 records, since there are a lot of them there\n",
    "    for raw_record in raw_dataset.take(5):  \n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "        # Print the parsed example \n",
    "        print(example)  \n",
    "\n",
    "# Test the function on 001.tfrecord\n",
    "parse_tfrecord('tfrfile/001.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a943a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
